---
phase: 2.1-position-polling
plan: 03
type: execute
depends_on: ["2.1-01", "2.1-02"]
files_modified: [src/v6/data/queue_worker.py]
domain: position-manager
inserted: true
urgency: critical
---

<objective>
Implement QueueWorker background daemon to process queued position updates in batches.

**Purpose:** Process non-essential contracts from PositionQueue in batches (every 5 seconds, 50 items per batch) without consuming streaming slots.

**Output:** Working QueueWorker daemon that fetches queued positions, updates Delta Lake, and marks items as processed.

**Key Pattern:** Background daemon with periodic batch processing (follows IB_CENTRAL_MANAGER_DESIGN.md daemon pattern).
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/2-position-synchronization/STREAMING_SLOTS_ISSUE.md
@.planning/phases/2.1-position-polling/2.1-OVERVIEW.md
@.planning/phases/2.1-position-polling/2.1-01-SUMMARY.md
@.planning/phases/2.1-position-polling/2.1-02-SUMMARY.md
@v5/IB_CENTRAL_MANAGER_DESIGN.md
@src/v6/data/position_queue.py
@src/v6/data/delta_persistence.py

**QueueWorker Purpose:**
- Background daemon that processes PositionQueue
- Fetches batch of queued items (priority 2, limit 50)
- Fetches position data from IB using `reqPositionsAsync()`
- Updates Delta Lake via DeltaLakePositionWriter (or direct write)
- Marks items as SUCCESS or FAILED
- Runs every 5 seconds (configurable)

**Processing Flow:**
```
while running:
  await asyncio.sleep(interval)

  batch = await queue.get_batch(priority=2, limit=50)

  for item in batch:
    position = await ib.reqPositionsAsync(item.conid)
    await delta_writer.write(position)
    await queue.mark_success(item.request_id)
```

**Tech Stack:**
- asyncio (background daemon with periodic processing)
- PositionQueue (get batch of items)
- ib_async (reqPositionsAsync for position data)
- DeltaLakePositionWriter (persist to Delta Lake)
- Delta Lake (position_updates table)

**Established Patterns:**
- Background daemon pattern (from IB_CENTRAL_MANAGER_DESIGN.md)
- Batch processing (50 items per cycle)
- Error handling and retry logic
- Async lifecycle (start/stop methods)

**Constraining Decisions:**
- **Interval:** 5 seconds between batches (configurable)
- **Batch size:** 50 items per batch
- **Error handling:** Mark failed items, continue processing
- **IB connection:** Share single connection (via IBConnectionManager)

</context>

<tasks>

<task type="auto">
  <name>Task 1: Create QueueWorker background daemon</name>
  <files>src/v6/data/queue_worker.py</files>
  <action>
    Create `src/v6/data/queue_worker.py`:

    ```python
    """
    Queue Worker for processing queued position updates.

    Background daemon that processes non-essential contracts from PositionQueue
    in batches. Fetches position data from IB and updates Delta Lake.

    Follows IB_CENTRAL_MANAGER_DESIGN.md daemon pattern.
    """

    import asyncio
    from dataclasses import dataclass
    from datetime import datetime
    from pathlib import Path
    from typing import List, Optional

    from loguru import logger
    import polars as pl

    from v6.data.position_queue import PositionQueue, QueuedPosition, QueueStatus
    from v6.data.delta_persistence import DeltaLakePositionWriter
    from v6.utils.ib_connection import IBConnectionManager

    @dataclass(slots=True)
    class WorkerStats:
        """Statistics for queue worker."""
        total_processed: int = 0
        total_success: int = 0
        total_failed: int = 0
        last_batch_time: Optional[datetime] = None
        last_batch_size: int = 0

    class QueueWorker:
        """
        Background daemon for processing queued position updates.

        **Purpose:**
        Process non-essential contracts from PositionQueue in batches.
        Fetches position data from IB using reqPositionsAsync().
        Updates Delta Lake with latest position information.

        **Pattern:**
        Follows IB_CENTRAL_MANAGER_DESIGN.md daemon pattern:
        - Periodic batch processing (every 5 seconds)
        - Error handling with retry logic
        - Status tracking (SUCCESS/FAILED)

        **Slot Conservation:**
        Uses reqPositionsAsync() (polling) instead of reqMktData() (streaming).
        Consumes 0 streaming slots.
        """

        def __init__(
            self,
            queue: PositionQueue,
            interval: int = 5,
            batch_size: int = 50,
            delta_lake_path: str = "data/lake/position_updates"
        ):
            """
            Initialize queue worker.

            Args:
                queue: PositionQueue to process
                interval: Seconds between batches (default: 5s)
                batch_size: Max items per batch (default: 50)
                delta_lake_path: Path to position_updates Delta Lake table
            """
            self.queue = queue
            self.interval = interval
            self.batch_size = batch_size
            self.delta_lake_path = Path(delta_lake_path)

            self._connection = None
            self._task: Optional[asyncio.Task] = None
            self._is_running = False

            self.stats = WorkerStats()

        async def start(self) -> None:
            """
            Start queue worker daemon.

            Begins periodic batch processing of queued items.
            """
            if self._is_running:
                logger.warning("QueueWorker already running")
                return

            logger.info(f"Starting QueueWorker (interval: {self.interval}s, batch: {self.batch_size})")

            # Get IB connection
            conn_manager = IBConnectionManager()
            self._connection = await conn_manager.get_connection()

            if not self._connection or not self._connection.is_connected:
                raise ConnectionError("IB not connected")

            # Initialize queue
            await self.queue.initialize()

            self._is_running = True
            self._task = asyncio.create_task(self._worker_loop())

            logger.info("✓ QueueWorker started")

        async def stop(self) -> None:
            """
            Stop queue worker daemon.

            Gracefully stops batch processing.
            """
            self._is_running = False

            if self._task and not self._task.done():
                self._task.cancel()
                try:
                    await self._task
                except asyncio.CancelledError:
                    pass

            logger.info("✓ QueueWorker stopped")

        async def _worker_loop(self) -> None:
            """
            Periodic batch processing loop.

            Every interval seconds:
            1. Get batch from queue (priority 2, limit batch_size)
            2. Fetch positions from IB
            3. Update Delta Lake
            4. Mark items as SUCCESS/FAILED
            """
            while self._is_running:
                try:
                    await asyncio.sleep(self.interval)

                    if self._is_running:
                        await self._process_batch()

                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"Worker loop error: {e}", exc_info=True)

        async def _process_batch(self) -> None:
            """
            Process one batch of queued items.

            Args:
                batch: List of QueuedPosition items
            """
            # Get batch from queue
            batch = await self.queue.get_batch(
                priority=2,
                limit=self.batch_size,
                status=QueueStatus.PENDING
            )

            if not batch:
                logger.debug("No queued items to process")
                return

            logger.info(f"Processing batch of {len(batch)} queued items")
            start_time = datetime.now()

            # Fetch positions from IB (all at once for efficiency)
            ib = self._connection.ib
            all_positions = await ib.reqPositionsAsync()

            # Build map for fast lookup: conid -> position
            position_map = {}
            for item in all_positions:
                if hasattr(item, 'contract') and hasattr(item.contract, 'conId'):
                    position_map[item.contract.conId] = item

            # Process each queued item
            success_ids = []
            failed_items = []

            for queued_item in batch:
                try:
                    # Find position data
                    if queued_item.conid not in position_map:
                        logger.warning(f"Position {queued_item.conid} not found in IB")
                        failed_items.append((queued_item, "Position not found in IB"))
                        continue

                    ib_position = position_map[queued_item.conid]

                    # Create PositionUpdate
                    from v6.data.position_streamer import PositionUpdate

                    update = PositionUpdate(
                        conid=queued_item.conid,
                        symbol=queued_item.symbol,
                        right=ib_position.contract.right if hasattr(ib_position.contract, 'right') else 'CALL',
                        position=ib_position.position,
                        market_price=ib_position.marketPrice,
                        market_value=ib_position.marketValue,
                        average_cost=ib_position.averageCost,
                        unrealized_pnl=ib_position.unrealizedPNL,
                        timestamp=datetime.now()
                    )

                    # Write to Delta Lake (direct write, not via DeltaLakePositionWriter)
                    await self._write_to_delta_lake([update])

                    success_ids.append(queued_item.request_id)
                    self.stats.total_success += 1

                except Exception as e:
                    logger.error(f"Failed to process {queued_item.request_id}: {e}")
                    failed_items.append((queued_item, str(e)))
                    self.stats.total_failed += 1

            # Mark items as SUCCESS/FAILED
            if success_ids:
                await self.queue.mark_success(success_ids)
                logger.debug(f"Marked {len(success_ids)} items as SUCCESS")

            for item, error in failed_items:
                await self.queue.mark_failed(item.request_id, error)
                logger.debug(f"Marked {item.request_id} as FAILED: {error}")

            # Update stats
            self.stats.total_processed += len(batch)
            self.stats.last_batch_time = datetime.now()
            self.stats.last_batch_size = len(batch)

            duration = (datetime.now() - start_time).total_seconds()
            logger.info(
                f"✓ Processed batch: {len(success_ids)} success, "
                f"{len(failed_items)} failed ({duration:.2f}s)"
            )

        async def _write_to_delta_lake(self, updates: List) -> None:
            """
            Write position updates to Delta Lake.

            Args:
                updates: List of PositionUpdate objects
            """
            try:
                import deltalake as dl

                # Convert to list of dicts
                data = [{
                    'conid': u.conid,
                    'symbol': u.symbol,
                    'right': u.right,
                    'strike': 0.0,  # Not available in PositionUpdate
                    'expiry': '',  # Not available in PositionUpdate
                    'position': u.position,
                    'market_price': u.market_price,
                    'market_value': u.market_value,
                    'average_cost': u.average_cost,
                    'unrealized_pnl': u.unrealized_pnl,
                    'timestamp': u.timestamp.isoformat(),
                    'date': u.timestamp.date()
                } for u in updates]

                df = pl.DataFrame(data)

                # Append to Delta Lake
                dl.write_deltalake(
                    str(self.delta_lake_path),
                    df,
                    mode='append'
                )

            except Exception as e:
                logger.error(f"Failed to write to Delta Lake: {e}")
                raise

        def get_stats(self) -> WorkerStats:
            """
            Get worker statistics.

            Returns:
                WorkerStats object with current statistics
            """
            return self.stats
    ```

    **CRITICAL:**
    - Background daemon with periodic processing (every 5 seconds)
    - Batch processing (50 items per cycle)
    - Uses reqPositionsAsync() (polling, 0 streaming slots)
    - Writes directly to Delta Lake (position_updates table)
    - Error handling with SUCCESS/FAILED marking
    - Statistics tracking for monitoring
  </action>
  <verify>
    Run `python -m py_compile src/v6/data/queue_worker.py` - should compile without errors
  </verify>
  <done>
    - QueueWorker class created with background daemon
    - Periodic batch processing (5-second intervals)
    - Fetches positions via reqPositionsAsync()
    - Updates Delta Lake with position data
    - Marks items as SUCCESS/FAILED
    - Statistics tracking implemented
    - Code compiles without errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Create integration test for QueueWorker</name>
  <files>src/v6/data/test_queue_worker.py</files>
  <action>
    Create `src/v6/data/test_queue_worker.py`:

    ```python
    """
    Integration test for QueueWorker.

    Tests background daemon processing of queued position updates.
    """

    import asyncio
    from loguru import logger

    from v6.data.position_queue import PositionQueue
    from v6.data.queue_worker import QueueWorker

    async def main():
        """Test QueueWorker."""
        logger.info("Testing QueueWorker...")

        # Create queue
        queue = PositionQueue(delta_lake_path="data/lake/test_position_queue")
        await queue.initialize()

        # Insert test items
        logger.info("Inserting test items into queue...")
        for i in range(5):
            await queue.insert(conid=300000 + i, symbol="TEST", priority=2)
        logger.info("✓ Inserted 5 test items")

        # Create worker
        worker = QueueWorker(
            queue=queue,
            interval=2,  # 2 seconds for testing
            batch_size=10,
            delta_lake_path="data/lake/test_position_updates"
        )

        try:
            # Start worker
            await worker.start()
            logger.info("✓ QueueWorker started")

            # Wait for one processing cycle
            await asyncio.sleep(5)

            # Get stats
            stats = worker.get_stats()
            logger.info(f"\n=== Worker Stats ===")
            logger.info(f"Total processed: {stats.total_processed}")
            logger.info(f"Total success: {stats.total_success}")
            logger.info(f"Total failed: {stats.total_failed}")
            logger.info(f"Last batch size: {stats.last_batch_size}")

            # Stop worker
            await worker.stop()
            logger.info("✓ QueueWorker stopped")

            # Verify queue is processed
            remaining = await queue.get_batch(priority=2, limit=100)
            logger.info(f"\nRemaining in queue: {len(remaining)}")

            logger.info("\n✓ Test complete")

        except Exception as e:
            logger.error(f"Test failed: {e}")
            logger.info("Note: This test requires IB Gateway/TWS to be running")

    if __name__ == "__main__":
        asyncio.run(main())
    ```

    **Note:** Test uses 2-second interval (faster than 5s production) for quick testing.
  </action>
  <verify>
    Run `python -m py_compile src/v6/data/test_queue_worker.py` - should compile without errors
  </verify>
  <done>
    - Integration test created for QueueWorker
    - Tests background daemon processing
    - Tests batch processing and statistics
    - Code compiles without errors
  </done>
</task>

<task type="auto">
  <name>Task 3: Update data package exports</name>
  <files>src/v6/data/__init__.py</files>
  <action>
    Update `src/v6/data/__init__.py` to export QueueWorker:

    ```python
    from v6.data.position_streamer import (
        PositionUpdate,
        PositionUpdateHandler,
        IBPositionStreamer,
    )
    from v6.data.delta_persistence import (
        PositionUpdatesTable,
        DeltaLakePositionWriter,
    )
    from v6.data.reconciliation import (
        DiscrepancyType,
        Discrepancy,
        ReconciliationResult,
        PositionReconciler,
        ReconciliationService,
    )
    from v6.data.strategy_registry import (
        ActiveContract,
        StrategyRegistry,
    )
    from v6.data.position_queue import (
        QueueStatus,
        QueuedPosition,
        PositionQueue,
    )
    from v6.data.queue_worker import (
        WorkerStats,
        QueueWorker,
    )

    __all__ = [
        # Position streaming
        "PositionUpdate",
        "PositionUpdateHandler",
        "IBPositionStreamer",
        # Delta Lake persistence
        "PositionUpdatesTable",
        "DeltaLakePositionWriter",
        # Reconciliation
        "DiscrepancyType",
        "Discrepancy",
        "ReconciliationResult",
        "PositionReconciler",
        "ReconciliationService",
        # Strategy registry
        "ActiveContract",
        "StrategyRegistry",
        # Position queue
        "QueueStatus",
        "QueuedPosition",
        "PositionQueue",
        # Queue worker
        "WorkerStats",
        "QueueWorker",
    ]
    ```

    This enables clean imports: `from v6.data import QueueWorker`
  </action>
  <verify>
    Run `python -c "from v6.data import QueueWorker; print('✓ Imports work')"` - should work without errors
  </verify>
  <done>
    - __init__.py updated with QueueWorker exports
    - All components accessible from v6.data package
    - Imports work without errors
  </done>
</task>

<task type="auto">
  <name>Task 4: Run integration test</name>
  <files>src/v6/data/test_queue_worker.py</files>
  <action>
    Run integration test to verify QueueWorker works:

    ```bash
    rm -rf data/lake/test_position_queue data/lake/test_position_updates
    PYTHONPATH=/home/bigballs/project/bot/v6/src python src/v6/data/test_queue_worker.py
    ```

    **Expected output:**
    ```
    Testing QueueWorker...
    Inserting test items into queue...
    ✓ Inserted 5 test items
    ✓ QueueWorker started
    ✓ Processed batch: 5 success, 0 failed (X.XXs)
    ✓ QueueWorker stopped

    === Worker Stats ===
    Total processed: 5
    Total success: 5
    Total failed: 0
    Last batch size: 5

    Remaining in queue: 0

    ✓ Test complete
    ```

    **Note:** Requires IB Gateway/TWS to be running.
  </action>
  <verify>
    Test runs successfully, all assertions pass
  </verify>
  <done>
    - Integration test executed successfully
    - QueueWorker processes queued items
    - Statistics tracking works correctly
    - All tests pass
  </done>
</task>

<task type="auto">
  <name>Task 5: Run ruff linter and create SUMMARY</name>
  <files>src/v6/data/queue_worker.py</files>
  <action>
    Run ruff linter and fix any issues:

    ```bash
    ruff check src/v6/data/queue_worker.py src/v6/data/test_queue_worker.py
    ```

    If issues found, run `ruff check --fix` to auto-fix.

    Then create SUMMARY.md as specified in plan output.
  </action>
  <verify>
    ruff linter passes with no errors
  </verify>
  <done>
    - ruff linter passes with no errors
    - SUMMARY.md created with all details
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -m py_compile src/v6/data/queue_worker.py` succeeds without errors
- [ ] `python -m py_compile src/v6/data/test_queue_worker.py` succeeds without errors
- [ ] QueueWorker has start() and stop() methods
- [ ] QueueWorker processes batch every 5 seconds (configurable)
- [ ] QueueWorker uses reqPositionsAsync() for fetching positions
- [ ] QueueWorker writes to Delta Lake position_updates table
- [ ] QueueWorker marks items as SUCCESS/FAILED
- [ ] Statistics tracking implemented
- [ ] Integration test runs successfully
- [ ] ruff linter passes with no errors
</verification>

<success_criteria>

- QueueWorker class created with background daemon
- Periodic batch processing (5-second intervals, 50 items per batch)
- Uses reqPositionsAsync() for polling (0 streaming slots)
- Updates Delta Lake with position data
- Marks items as SUCCESS/FAILED
- Statistics tracking for monitoring
- Integration test verifies functionality
- All verification checks pass
- No errors or warnings introduced

</success_criteria>

<output>
After completion, create `.planning/phases/2.1-position-polling/2.1-03-SUMMARY.md`:

# Phase 2.1 Plan 3: QueueWorker Background Daemon Summary

**Implemented QueueWorker background daemon to process queued position updates in batches.**

## Accomplishments

- Created QueueWorker with background daemon pattern
- Periodic batch processing (5-second intervals, 50 items per batch)
- Uses reqPositionsAsync() for polling (0 streaming slots consumed)
- Updates Delta Lake position_updates table
- Marks items as SUCCESS/FAILED based on processing result
- Statistics tracking for monitoring (total processed, success, failed)
- Integration test verifies functionality

## Files Created/Modified

- `src/v6/data/queue_worker.py` - QueueWorker daemon class
- `src/v6/data/test_queue_worker.py` - Integration test script
- `src/v6/data/__init__.py` - Updated exports

## Decisions Made

- **Interval:** 5 seconds between batches (configurable via constructor)
- **Batch size:** 50 items per batch (configurable)
- **Polling method:** reqPositionsAsync() (0 streaming slots)
- **Delta Lake writes:** Direct writes to position_updates table
- **Error handling:** Mark failed items, continue processing
- **Statistics:** Track total_processed, total_success, total_failed

## Phase 2.1 Status

**Plans Complete:**
- ✅ 2.1-01: StrategyRegistry and PositionQueue
- ✅ 2.1-02: IBPositionStreamer Hybrid Redesign
- ✅ 2.1-03: QueueWorker Background Daemon
- ⏳ 2.1-04: Integration Testing and Documentation (next)

## Next Step

Ready for 2.1-04-PLAN.md (Integration Testing and Documentation)

The hybrid position synchronization is fully implemented:
- StrategyRegistry tracks active contracts
- IBPositionStreamer routes active→stream, non-active→queue
- QueueWorker processes queue in batches
- Delta Lake updated with position data
- Streaming slots conserved (0 for non-essential contracts)

Next: End-to-end integration testing and documentation.

---

**Plan:** 2.1-03-PLAN.md
**Tasks completed:** 5/5
**Deviations encountered:** none
**Commits:** X (X feature commits)
**Status:** COMPLETE
</output>
