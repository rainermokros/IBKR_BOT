---
phase: 2.1-position-polling
plan: 01
type: execute
depends_on: ["02-01", "02-02", "02-03"]
files_modified: [src/v6/data/strategy_registry.py, src/v6/data/position_queue.py]
domain: position-manager
inserted: true
urgency: critical
---

<objective>
Create foundation for hybrid position synchronization: StrategyRegistry to track active contracts and PositionQueue to manage queued position updates.

**Purpose:** Enable hybrid approach where active contracts are streamed (real-time) while non-essential contracts are queued (0 slots consumed).

**Output:** Working StrategyRegistry class and PositionQueue Delta Lake table with queue operations.

**Key Pattern:** Delta Lake as persistent queue (survives reboots) + priority-based processing (active > monitoring).
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/2-position-synchronization/STREAMING_SLOTS_ISSUE.md
@.planning/phases/2-position-synchronization/2-01-SUMMARY.md
@.planning/phases/2-position-synchronization/2-02-SUMMARY.md
@.planning/phases/2.1-position-polling/2.1-OVERVIEW.md
@v5/IB_CENTRAL_MANAGER_DESIGN.md

**Hybrid Architecture:**
- **Active contracts** → STREAM (reqMktData, real-time, consume slots)
- **Non-essential contracts** → QUEUE (Delta Lake table, processed later, 0 slots)

**StrategyRegistry Purpose:**
- Track which contracts are in active strategies (Iron Condors, spreads, etc.)
- Fast in-memory lookup: `is_active(conid)`
- Sync with Delta Lake: active_strategies table
- API for strategy builders: `add_active()`, `remove_active()`

**PositionQueue Purpose:**
- Delta Lake table as persistent queue (follows IB_CENTRAL_MANAGER_DESIGN.md pattern)
- Queue non-essential contracts for batch processing
- Schema: request_id, conid, priority, status, created_at, updated_at
- Operations: insert(), get_batch(), mark_processed()

**Tech Stack:**
- Polars (data manipulation)
- Delta Lake (persistent queue storage)
- asyncio (async queue operations)
- ib_async (contract serialization)

**Established Patterns:**
- Delta Lake tables with timestamp partitioning (Phase 1-02)
- Repository pattern for data access (Phase 1-04)
- Dataclasses with slots=True (Phase 1-04)

**Constraining Decisions:**
- **Priority 1:** Active strategy contracts (streamed directly, don't queue)
- **Priority 2:** Monitoring contracts (queued for batch processing)
- **Batch size:** 50 contracts per queue processing cycle
- **Queue interval:** 5 seconds between batches (configurable)

</context>

<tasks>

<task type="auto">
  <name>Task 1: Create StrategyRegistry class</name>
  <files>src/v6/data/strategy_registry.py</files>
  <action>
    Create `src/v6/data/strategy_registry.py`:

    ```python
    """
    Strategy Registry for tracking active option contracts.

    This registry tracks which contracts are in active strategies (Iron Condors,
    vertical spreads, etc.) to determine which contracts should be streamed
    (real-time) vs queued (batch processing).

    Active contracts consume streaming slots but get immediate updates.
    Non-active contracts are queued and processed periodically (0 slots).
    """

    import asyncio
    from dataclasses import dataclass, field
    from datetime import datetime
    from pathlib import Path
    from typing import Set, Optional

    from loguru import logger
    import polars as pl

    @dataclass(slots=True)
    class ActiveContract:
        """An active option contract in a strategy."""
        conid: int
        symbol: str
        right: str  # CALL or PUT
        strike: float
        expiry: str
        strategy_id: int
        added_at: datetime = field(default_factory=datetime.now)

    class StrategyRegistry:
        """
        Registry of active option contracts.

        Tracks which contracts are in active strategies to determine
        streaming vs queueing decision.

        **Two-Tier Storage:**
        1. In-memory: Fast lookup (self._active_contracts)
        2. Delta Lake: Persistent storage (active_strategies table)

        **Thread-Safe:**
        Uses asyncio.Lock for concurrent access protection.
        """

        def __init__(self, delta_lake_path: str = "data/lake/active_strategies"):
            """
            Initialize registry.

            Args:
                delta_lake_path: Path to Delta Lake table for persistence
            """
            self.delta_lake_path = Path(delta_lake_path)
            self._active_contracts: dict[int, ActiveContract] = {}
            self._lock = asyncio.Lock()
            self._initialized = False

        async def initialize(self) -> None:
            """
            Load active contracts from Delta Lake on startup.

            Called once at system startup to rebuild in-memory cache
            from persistent storage.
            """
            if self._initialized:
                return

            async with self._lock:
                # Check if table exists
                if not self.delta_lake_path.exists():
                    logger.info("No active strategies table found (empty registry)")
                    self._initialized = True
                    return

                # Load from Delta Lake
                try:
                    import deltalake as dl

                    dt = dl.DeltaTable(str(self.delta_lake_path))
                    df_pl = pl.from_pandas(dt.to_pandas())

                    # Build in-memory cache
                    for row in df_pl.to_dicts():
                        self._active_contracts[row['conid']] = ActiveContract(
                            conid=row['conid'],
                            symbol=row['symbol'],
                            right=row['right'],
                            strike=row['strike'],
                            expiry=row['expiry'],
                            strategy_id=row['strategy_id'],
                            added_at=datetime.fromisoformat(row['added_at'])
                        )

                    logger.info(f"✓ Loaded {len(self._active_contracts)} active contracts from registry")
                    self._initialized = True

                except Exception as e:
                    logger.error(f"Failed to load active strategies: {e}")
                    self._initialized = True

        def is_active(self, conid: int) -> bool:
            """
            Check if contract is in active strategy.

            Args:
                conid: IB contract ID

            Returns:
                True if contract should be streamed (active strategy)
                False if contract should be queued (non-essential)
            """
            return conid in self._active_contracts

        async def add_active(
            self,
            conid: int,
            symbol: str,
            right: str,
            strike: float,
            expiry: str,
            strategy_id: int
        ) -> None:
            """
            Add contract to active registry.

            Called when strategy enters a position.

            Args:
                conid: IB contract ID
                symbol: Underlying symbol
                right: CALL or PUT
                strike: Strike price
                expiry: Expiration date (IB format)
                strategy_id: Strategy ID this contract belongs to
            """
            async with self._lock:
                # Create active contract record
                contract = ActiveContract(
                    conid=conid,
                    symbol=symbol,
                    right=right,
                    strike=strike,
                    expiry=expiry,
                    strategy_id=strategy_id
                )

                # Add to in-memory cache
                self._active_contracts[conid] = contract

                # Persist to Delta Lake
                await self._persist_contract(contract)

                logger.info(f"✓ Added active contract: {symbol} {right} {strike} (conid: {conid})")

        async def remove_active(self, conid: int) -> None:
            """
            Remove contract from active registry.

            Called when strategy exits a position.

            Args:
                conid: IB contract ID to remove
            """
            async with self._lock:
                if conid not in self._active_contracts:
                    logger.warning(f"Contract {conid} not in active registry")
                    return

                # Remove from in-memory cache
                del self._active_contracts[conid]

                # Update Delta Lake (soft delete: set removed_at timestamp)
                await self._mark_removed(conid)

                logger.info(f"✓ Removed active contract: {conid}")

        async def get_all_active(self) -> list[ActiveContract]:
            """
            Get all active contracts.

            Returns:
                List of all active contracts
            """
            async with self._lock:
                return list(self._active_contracts.values())

        async def _persist_contract(self, contract: ActiveContract) -> None:
            """
            Persist contract to Delta Lake.

            Args:
                contract: ActiveContract to persist
            """
            try:
                import deltalake as dl

                # Create table if not exists
                if not self.delta_lake_path.exists():
                    self._create_table()

                # Insert record
                df = pl.DataFrame([{
                    'conid': contract.conid,
                    'symbol': contract.symbol,
                    'right': contract.right,
                    'strike': contract.strike,
                    'expiry': contract.expiry,
                    'strategy_id': contract.strategy_id,
                    'added_at': contract.added_at.isoformat(),
                    'removed_at': None
                }])

                dl.write_deltalake(
                    str(self.delta_lake_path),
                    df,
                    mode='append'
                )

            except Exception as e:
                logger.error(f"Failed to persist active contract: {e}")

        async def _mark_removed(self, conid: int) -> None:
            """
            Mark contract as removed in Delta Lake.

            Args:
                conid: Contract ID to mark removed
            """
            try:
                import deltalake as dl

                dt = dl.DeltaTable(str(self.delta_lake_path))

                # Update removed_at timestamp
                dt.update(
                    predicate=f"conid = {conid}",
                    updates={"removed_at": datetime.now().isoformat()}
                )

            except Exception as e:
                logger.error(f"Failed to mark contract removed: {e}")

        def _create_table(self) -> None:
            """Create active_strategies Delta Lake table."""
            import deltalake as dl

            # Define schema
            schema = pl.Schema({
                'conid': pl.Int64,
                'symbol': pl.String,
                'right': pl.String,
                'strike': pl.Float64,
                'expiry': pl.String,
                'strategy_id': pl.Int64,
                'added_at': pl.String,
                'removed_at': pl.Optional[pl.String]
            })

            # Create empty table
            df = pl.DataFrame(schema=schema)

            dl.write_deltalake(
                str(self.delta_lake_path),
                df,
                mode='overwrite'
            )

            logger.info(f"✓ Created active_strategies table: {self.delta_lake_path}")
    ```

    **CRITICAL:**
    - Two-tier storage: in-memory (fast) + Delta Lake (persistent)
    - Thread-safe with asyncio.Lock
    - `is_active(conid)` for fast lookup
    - `add_active()` called when strategy enters position
    - `remove_active()` called when strategy exits position
    - Survives reboots via Delta Lake persistence
  </action>
  <verify>
    Run `python -m py_compile src/v6/data/strategy_registry.py` - should compile without errors
  </verify>
  <done>
    - StrategyRegistry class created with in-memory + Delta Lake storage
    - is_active() method for fast contract lookup
    - add_active() and remove_active() for lifecycle management
    - asyncio.Lock for thread safety
    - Code compiles without errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Create PositionQueue Delta Lake table and class</name>
  <files>src/v6/data/position_queue.py</files>
  <action>
    Create `src/v6/data/position_queue.py`:

    ```python
    """
    Position Queue for batch processing of non-essential contracts.

    Implements persistent queue using Delta Lake (follows IB_CENTRAL_MANAGER_DESIGN.md pattern).
    Non-essential contracts are queued and processed in batches (0 streaming slots consumed).
    """

    import uuid
    from dataclasses import dataclass
    from datetime import datetime
    from enum import Enum
    from pathlib import Path
    from typing import List, Optional

    import polars as pl
    from loguru import logger

    class QueueStatus(str, Enum):
        """Status of queued position update."""
        PENDING = "PENDING"
        PROCESSING = "PROCESSING"
        SUCCESS = "SUCCESS"
        FAILED = "FAILED"

    @dataclass(slots=True)
    class QueuedPosition:
        """A position update waiting in queue."""
        request_id: str
        conid: int
        symbol: str
        priority: int  # 1=active (streamed), 2=monitoring (queued)
        status: QueueStatus
        created_at: datetime
        updated_at: datetime
        error_message: Optional[str] = None

    class PositionQueue:
        """
        Persistent queue for position updates using Delta Lake.

        Non-essential contracts are queued here and processed in batches by QueueWorker.
        This conserves IB streaming slots (0 slots consumed for queued contracts).

        **Schema:**
        - request_id: UUID (unique identifier)
        - conid: IB contract ID
        - symbol: Underlying symbol
        - priority: 1 (active/streamed), 2 (monitoring/queued)
        - status: PENDING, PROCESSING, SUCCESS, FAILED
        - created_at: Timestamp when queued
        - updated_at: Timestamp of last status change
        - error_message: Error details if failed
        """

        def __init__(self, delta_lake_path: str = "data/lake/position_queue"):
            """
            Initialize queue.

            Args:
                delta_lake_path: Path to Delta Lake table
            """
            self.delta_lake_path = Path(delta_lake_path)
            self._initialized = False

        async def initialize(self) -> None:
            """Initialize queue (create table if not exists)."""
            if self._initialized:
                return

            if not self.delta_lake_path.exists():
                self._create_table()
                logger.info(f"✓ Created position_queue table: {self.delta_lake_path}")

            self._initialized = True

        async def insert(
            self,
            conid: int,
            symbol: str,
            priority: int = 2
        ) -> str:
            """
            Insert position update into queue.

            Args:
                conid: IB contract ID
                symbol: Underlying symbol
                priority: Priority level (default: 2 for monitoring)

            Returns:
                request_id: UUID of queued item
            """
            # Ensure queue exists
            await self.initialize()

            # Generate request ID
            request_id = str(uuid.uuid4())
            now = datetime.now()

            # Create record
            record = {
                'request_id': request_id,
                'conid': conid,
                'symbol': symbol,
                'priority': priority,
                'status': QueueStatus.PENDING.value,
                'created_at': now.isoformat(),
                'updated_at': now.isoformat(),
                'error_message': None
            }

            # Insert into Delta Lake
            try:
                import deltalake as dl

                df = pl.DataFrame([record])
                dl.write_deltalake(
                    str(self.delta_lake_path),
                    df,
                    mode='append'
                )

                logger.debug(f"✓ Queued position update: {symbol} (conid: {conid}, priority: {priority})")
                return request_id

            except Exception as e:
                logger.error(f"Failed to insert into queue: {e}")
                raise

        async def get_batch(
            self,
            priority: int = 2,
            limit: int = 50,
            status: QueueStatus = QueueStatus.PENDING
        ) -> List[QueuedPosition]:
            """
            Get batch of queued items for processing.

            Args:
                priority: Priority level to fetch (default: 2)
                limit: Max items to return (default: 50)
                status: Status to filter (default: PENDING)

            Returns:
                List of QueuedPosition items
            """
            try:
                import deltalake as dl

                dt = dl.DeltaTable(str(self.delta_lake_path))
                df = pl.from_pandas(dt.to_pandas())

                # Filter and sort
                batch_df = df.filter(
                    (pl.col("priority") == priority) &
                    (pl.col("status") == status.value)
                ).sort("created_at").head(limit)

                # Update status to PROCESSING
                request_ids = batch_df["request_id"].to_list()
                await self._update_status(request_ids, QueueStatus.PROCESSING)

                # Convert to QueuedPosition objects
                result = []
                for row in batch_df.to_dicts():
                    result.append(QueuedPosition(
                        request_id=row["request_id"],
                        conid=row["conid"],
                        symbol=row["symbol"],
                        priority=row["priority"],
                        status=QueueStatus(row["status"]),
                        created_at=datetime.fromisoformat(row["created_at"]),
                        updated_at=datetime.fromisoformat(row["updated_at"]),
                        error_message=row.get("error_message")
                    ))

                logger.debug(f"✓ Retrieved {len(result)} items from queue (priority: {priority})")
                return result

            except Exception as e:
                logger.error(f"Failed to get batch from queue: {e}")
                return []

        async def mark_success(
            self,
            request_ids: List[str]
        ) -> None:
            """
            Mark queued items as successfully processed.

            Args:
                request_ids: List of request IDs to mark successful
            """
            await self._update_status(request_ids, QueueStatus.SUCCESS)
            logger.debug(f"✓ Marked {len(request_ids)} items as SUCCESS")

        async def mark_failed(
            self,
            request_id: str,
            error_message: str
        ) -> None:
            """
            Mark queued item as failed.

            Args:
                request_id: Request ID that failed
                error_message: Human-readable error message
            """
            await self._update_status(
                [request_id],
                QueueStatus.FAILED,
                error_message=error_message
            )
            logger.warning(f"✗ Marked {request_id} as FAILED: {error_message}")

        async def _update_status(
            self,
            request_ids: List[str],
            status: QueueStatus,
            error_message: Optional[str] = None
        ) -> None:
            """
            Update status of queued items.

            Args:
                request_ids: List of request IDs to update
                status: New status
                error_message: Optional error message
            """
            try:
                import deltalake as dl

                dt = dl.DeltaTable(str(self.delta_lake_path))

                # Build update dict
                updates = {
                    "status": status.value,
                    "updated_at": datetime.now().isoformat()
                }

                if error_message:
                    updates["error_message"] = error_message

                # Update matching records
                # Note: Delta Lake MERGE not available in Python API, use update with predicate
                for request_id in request_ids:
                    dt.update(
                        predicate=f"request_id = '{request_id}'",
                        updates=updates
                    )

            except Exception as e:
                logger.error(f"Failed to update queue status: {e}")

        def _create_table(self) -> None:
            """Create position_queue Delta Lake table."""
            import deltalake as dl

            # Define schema
            schema = pl.Schema({
                'request_id': pl.String,
                'conid': pl.Int64,
                'symbol': pl.String,
                'priority': pl.Int64,
                'status': pl.String,
                'created_at': pl.String,
                'updated_at': pl.String,
                'error_message': pl.Optional[pl.String]
            })

            # Create empty table
            df = pl.DataFrame(schema=schema)

            dl.write_deltalake(
                str(self.delta_lake_path),
                df,
                mode='overwrite'
            )

            logger.info(f"✓ Created position_queue table: {self.delta_lake_path}")
    ```

    **CRITICAL:**
    - Persistent queue using Delta Lake (survives reboots)
    - Priority-based retrieval (priority 1 first, then 2)
    - Batch processing (50 items per batch)
    - Status tracking (PENDING → PROCESSING → SUCCESS/FAILED)
    - Follows IB_CENTRAL_MANAGER_DESIGN.md pattern
  </action>
  <verify>
    Run `python -m py_compile src/v6/data/position_queue.py` - should compile without errors
  </verify>
  <done>
    - PositionQueue class created with Delta Lake backing
    - insert() for adding items to queue
    - get_batch() for retrieving batch of items
    - mark_success() and mark_failed() for status updates
    - Code compiles without errors
  </done>
</task>

<task type="auto">
  <name>Task 3: Create integration test for StrategyRegistry and PositionQueue</name>
  <files>src/v6/data/test_registry_and_queue.py</files>
  <action>
    Create `src/v6/data/test_registry_and_queue.py`:

    ```python
    """
    Integration test for StrategyRegistry and PositionQueue.

    Tests the foundation of hybrid position synchronization.
    """

    import asyncio
    from loguru import logger

    from v6.data.strategy_registry import StrategyRegistry
    from v6.data.position_queue import PositionQueue, QueueStatus

    async def main():
        """Test StrategyRegistry and PositionQueue."""
        logger.info("Testing StrategyRegistry and PositionQueue...")

        # Test StrategyRegistry
        logger.info("\n=== Testing StrategyRegistry ===")
        registry = StrategyRegistry(delta_lake_path="data/lake/test_active_strategies")

        # Initialize (create table)
        await registry.initialize()
        logger.info("✓ Registry initialized")

        # Add active contracts
        await registry.add_active(
            conid=123456,
            symbol="SPY",
            right="CALL",
            strike=450.0,
            expiry="20260220",
            strategy_id=1
        )
        logger.info("✓ Added active contract 123456")

        await registry.add_active(
            conid=123457,
            symbol="SPY",
            right="PUT",
            strike=440.0,
            expiry="20260220",
            strategy_id=1
        )
        logger.info("✓ Added active contract 123457")

        # Check is_active
        assert registry.is_active(123456), "Contract 123456 should be active"
        assert registry.is_active(123457), "Contract 123457 should be active"
        assert not registry.is_active(999999), "Contract 999999 should not be active"
        logger.info("✓ is_active() working correctly")

        # Get all active
        active = await registry.get_all_active()
        assert len(active) == 2, f"Should have 2 active contracts, got {len(active)}"
        logger.info(f"✓ get_all_active() returned {len(active)} contracts")

        # Remove active
        await registry.remove_active(123456)
        assert not registry.is_active(123456), "Contract 123456 should no longer be active"
        logger.info("✓ remove_active() working correctly")

        # Test PositionQueue
        logger.info("\n=== Testing PositionQueue ===")
        queue = PositionQueue(delta_lake_path="data/lake/test_position_queue")

        # Initialize
        await queue.initialize()
        logger.info("✓ Queue initialized")

        # Insert items
        request_id_1 = await queue.insert(conid=234567, symbol="SPY", priority=2)
        logger.info(f"✓ Inserted item {request_id_1}")

        request_id_2 = await queue.insert(conid=234568, symbol="SPY", priority=2)
        logger.info(f"✓ Inserted item {request_id_2}")

        # Get batch
        batch = await queue.get_batch(priority=2, limit=10)
        assert len(batch) == 2, f"Should retrieve 2 items, got {len(batch)}"
        logger.info(f"✓ Retrieved batch of {len(batch)} items")

        # Mark as success
        await queue.mark_success([item.request_id for item in batch])
        logger.info("✓ Marked items as SUCCESS")

        # Verify no more PENDING items
        batch = await queue.get_batch(priority=2, limit=10)
        assert len(batch) == 0, f"Should have 0 pending items, got {len(batch)}"
        logger.info("✓ No more PENDING items (all processed)")

        logger.info("\n✓ All tests passed!")

    if __name__ == "__main__":
        asyncio.run(main())
    ```

    **Note:** This test creates separate test tables (test_active_strategies, test_position_queue) to avoid polluting production data.
  </action>
  <verify>
    Run `python -m py_compile src/v6/data/test_registry_and_queue.py` - should compile without errors
  </verify>
  <done>
    - Integration test created for StrategyRegistry
    - Integration test created for PositionQueue
    - Tests add_active(), remove_active(), is_active()
    - Tests insert(), get_batch(), mark_success()
    - Code compiles without errors
  </done>
</task>

<task type="auto">
  <name>Task 4: Update data package exports</name>
  <files>src/v6/data/__init__.py</files>
  <action>
    Update `src/v6/data/__init__.py` to export new components:

    ```python
    from v6.data.position_streamer import (
        PositionUpdate,
        PositionUpdateHandler,
        IBPositionStreamer,
    )
    from v6.data.delta_persistence import (
        PositionUpdatesTable,
        DeltaLakePositionWriter,
    )
    from v6.data.reconciliation import (
        DiscrepancyType,
        Discrepancy,
        ReconciliationResult,
        PositionReconciler,
        ReconciliationService,
    )
    from v6.data.strategy_registry import (
        ActiveContract,
        StrategyRegistry,
    )
    from v6.data.position_queue import (
        QueueStatus,
        QueuedPosition,
        PositionQueue,
    )

    __all__ = [
        # Position streaming
        "PositionUpdate",
        "PositionUpdateHandler",
        "IBPositionStreamer",
        # Delta Lake persistence
        "PositionUpdatesTable",
        "DeltaLakePositionWriter",
        # Reconciliation
        "DiscrepancyType",
        "Discrepancy",
        "ReconciliationResult",
        "PositionReconciler",
        "ReconciliationService",
        # Strategy registry
        "ActiveContract",
        "StrategyRegistry",
        # Position queue
        "QueueStatus",
        "QueuedPosition",
        "PositionQueue",
    ]
    ```

    This enables clean imports: `from v6.data import StrategyRegistry, PositionQueue`
  </action>
  <verify>
    Run `python -c "from v6.data import StrategyRegistry, PositionQueue; print('✓ Imports work')"` - should work without errors
  </verify>
  <done>
    - __init__.py updated with registry and queue exports
    - All components accessible from v6.data package
    - Imports work without errors
  </done>
</task>

<task type="auto">
  <name>Task 5: Run integration test</name>
  <files>src/v6/data/test_registry_and_queue.py</files>
  <action>
    Run integration test to verify StrategyRegistry and PositionQueue work:

    ```bash
    rm -rf data/lake/test_active_strategies data/lake/test_position_queue
    PYTHONPATH=/home/bigballs/project/bot/v6/src python src/v6/data/test_registry_and_queue.py
    ```

    **Expected output:**
    ```
    Testing StrategyRegistry and PositionQueue...
    ✓ Registry initialized
    ✓ Added active contract 123456
    ✓ Added active contract 123457
    ✓ is_active() working correctly
    ✓ get_all_active() returned 2 contracts
    ✓ remove_active() working correctly

    === Testing PositionQueue ===
    ✓ Queue initialized
    ✓ Inserted item <uuid1>
    ✓ Inserted item <uuid2>
    ✓ Retrieved batch of 2 items
    ✓ Marked items as SUCCESS
    ✓ No more PENDING items (all processed)

    ✓ All tests passed!
    ```
  </action>
  <verify>
    Test runs successfully, all assertions pass
  </verify>
  <done>
    - Integration test executed successfully
    - StrategyRegistry operations verified
    - PositionQueue operations verified
    - All tests pass
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -m py_compile src/v6/data/strategy_registry.py` succeeds without errors
- [ ] `python -m py_compile src/v6/data/position_queue.py` succeeds without errors
- [ ] `python -m py_compile src/v6/data/test_registry_and_queue.py` succeeds without errors
- [ ] StrategyRegistry has is_active() method
- [ ] StrategyRegistry has add_active() and remove_active() methods
- [ ] StrategyRegistry uses Delta Lake for persistence
- [ ] PositionQueue has insert() method
- [ ] PositionQueue has get_batch() method
- [ ] PositionQueue has mark_success() and mark_failed() methods
- [ ] PositionQueue uses Delta Lake for persistence
- [ ] Integration test runs successfully
- [ ] ruff linter passes with no errors
</verification>

<success_criteria>

- StrategyRegistry class created with in-memory + Delta Lake storage
- is_active(conid) returns True for active contracts, False otherwise
- add_active() adds contract to registry and persists to Delta Lake
- remove_active() removes contract from registry and updates Delta Lake
- PositionQueue class created with Delta Lake backing
- insert() adds item to queue with priority
- get_batch() retrieves batch of PENDING items (priority 2, limit 50)
- mark_success() updates status to SUCCESS
- mark_failed() updates status to FAILED with error message
- Integration test verifies all functionality
- All verification checks pass
- No errors or warnings introduced

</success_criteria>

<output>
After completion, create `.planning/phases/2.1-position-polling/2.1-01-SUMMARY.md`:

# Phase 2.1 Plan 1: StrategyRegistry and PositionQueue Summary

**Created foundation for hybrid position synchronization: StrategyRegistry to track active contracts and PositionQueue to manage queued position updates.**

## Accomplishments

- Created StrategyRegistry with in-memory cache + Delta Lake persistence
- Created PositionQueue with Delta Lake backing (follows IB_CENTRAL_MANAGER_DESIGN.md pattern)
- Implemented fast contract lookup: `is_active(conid)`
- Implemented queue operations: insert(), get_batch(), mark_success(), mark_failed()
- Integration test verifies all functionality

## Files Created/Modified

- `src/v6/data/strategy_registry.py` - StrategyRegistry class for tracking active contracts
- `src/v6/data/position_queue.py` - PositionQueue class for batch processing queue
- `src/v6/data/test_registry_and_queue.py` - Integration test for both components
- `src/v6/data/__init__.py` - Updated exports

## Decisions Made

- **Two-tier storage:** In-memory (fast lookup) + Delta Lake (persistence)
- **Priority levels:** 1 (active/streamed), 2 (monitoring/queued)
- **Batch size:** 50 items per batch processing cycle
- **Queue interval:** 5 seconds (to be implemented in QueueWorker)
- **Thread safety:** asyncio.Lock for concurrent access protection

## Next Step

Ready for 2.1-02-PLAN.md (Redesign IBPositionStreamer with Hybrid Logic)

The foundation is in place:
- StrategyRegistry tracks which contracts are active
- PositionQueue manages queued updates for non-essential contracts
- Both use Delta Lake for persistence (survive reboots)

Next: Redesign IBPositionStreamer to use hybrid approach (stream active, queue non-active).

---

**Plan:** 2.1-01-PLAN.md
**Tasks completed:** 5/5
**Deviations encountered:** none
**Commits:** X (X feature commits)
**Status:** COMPLETE
</output>
